

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Pipeline (ML System) &mdash; PyKale 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> PyKale
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Installation (to update)</a></li>
</ul>
<p class="caption"><span class="caption-text">Core API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../kale.loaddata.html">Load data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kale.prepdata.html">Preprocess data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kale.embed.html">Embed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kale.predict.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kale.pipeline.html">Pipeline (ML System)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kale.utils.html">Utilities</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/examples.cifar_cnntransformer.html">CIFAR - CNN Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/examples.cifar_isonet.html">CIFAR - ISONet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/examples.digits_dann_lightn.html">Digits - Domain Adapatation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyKale</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Pipeline (ML System)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/backup/kale.pipeline.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="pipeline-ml-system">
<h1>Pipeline (ML System)<a class="headerlink" href="#pipeline-ml-system" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="kale-pipeline-da-systems-module">
<h2>kale.pipeline.da_systems module<a class="headerlink" href="#kale-pipeline-da-systems-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="kale-pipeline-video-transformer-module">
<h2>kale.pipeline.video_transformer module<a class="headerlink" href="#kale-pipeline-video-transformer-module" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-kale.pipeline.video_transformer"></span><dl class="py class">
<dt id="kale.pipeline.video_transformer.VideoTransformer">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.video_transformer.</code><code class="sig-name descname">VideoTransformer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fram_per_vid</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">cnntransformer</span><span class="p">:</span> <span class="n"><a class="reference internal" href="../kale.embed.html#kale.embed.attention_cnn.ContextCNNGeneric" title="kale.embed.attention_cnn.ContextCNNGeneric">kale.embed.attention_cnn.ContextCNNGeneric</a></span></em>, <em class="sig-param"><span class="n">cnntrans_out_shape</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>int<span class="p">, </span>int<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">nheads</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">linformer_k</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">dim_feedforward</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_layers</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">dropout</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">activation</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'relu'</span></em>, <em class="sig-param"><span class="n">pos_encoding</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.video_transformer.VideoTransformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<p>A feature extractor for videos consisting of three consecutive modules.</p>
<p>1. A CNN without a head that is applied independently on each frame of the
video.</p>
<p>2. A Transformer-Encoder that is applied independently on the output CNN-representation
of each frame after unrolling their remaining spatial dimensions into a sequence.</p>
<p>3. A Transformer-Encoder that is applied after concatenating each frame’s sequence
into one big sequence.</p>
<p>So, first the VideoTransformer extracts per-frame features through a CNN, then
per-frame contextualizes these features with self-attention, and finally globally
contextualizes these features, as a whole, with self-attention.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This module is a video-to-sequence model where a custom head
can be used ontop to customize this model for different types of tasks (or just
classification), in the same way BERT’s output sequences can be used for
various tasks.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fram_per_vid</strong> – the number of frames each input video has. This must always
be the same during training and inference (required).</p></li>
<li><p><strong>cnntransformer</strong> – a feature extractor built from kale.embed.attention_cnn
consisting of a cnn with a transformer-encoder stacked ontop
to process images (required).</p></li>
<li><p><strong>cnntrans_out_shape</strong> – the shape that the cnntransformer will output in
the format (seq_len, batch_size, channels) (required).</p></li>
<li><p><strong>nheads</strong> – number of attention heads in the <cite>step 3</cite> transformer-encoder (required).</p></li>
<li><p><strong>linformer_k</strong> – number of down projection dimensions of the <cite>step 3</cite>
transformer-encoder. See <cite>Linformer</cite> <a class="reference external" href="https://arxiv.org/abs/2006.04768">https://arxiv.org/abs/2006.04768</a>
for more details (required).</p></li>
<li><p><strong>dim_feedforward</strong> – number of neurons in the intermediate dense layer of
each <cite>step 3</cite> Transformer-Encoder feedforward block (required).</p></li>
<li><p><strong>num_layers</strong> – number of attention layers in the <cite>step 3</cite> Transformer-Encoder (required).</p></li>
<li><p><strong>dropout</strong> – dropout rate of the <cite>step 3</cite> Transformer-Encoder layers (default=0.1).</p></li>
<li><p><strong>activation</strong> – ‘relu’ or ‘gelu’ for the <cite>step 3</cite> Transformer-Encoder (default=’relu).</p></li>
<li><p><strong>pos_encoding</strong> – None, a custom positional-encoding block, or an identity block
that is applied on the final long sequences before <cite>step 3</cite>. If
None, the default sin-cos encodings will be applied (default=None).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fram_per_vid</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">channels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">fram_per_vid</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>                    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>                    <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cnn_output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contextualizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_type</span> <span class="o">=</span> <span class="s1">&#39;sequence&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cnntransformer</span> <span class="o">=</span> <span class="n">ContextCNNGeneric</span><span class="p">(</span><span class="n">cnn</span><span class="p">,</span> <span class="n">cnn_output_shape</span><span class="p">,</span> <span class="n">contextualizer</span><span class="p">,</span> <span class="n">output_type</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cnntrans_out_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">VideoTransformer</span><span class="p">(</span><span class="n">fram_per_vid</span><span class="p">,</span> <span class="n">cnntransformer</span><span class="p">,</span> <span class="n">cnntrans_out_shape</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">fram_per_vid</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">fram_per_vid</span><span class="o">*</span><span class="mi">8</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># True</span>
</pre></div>
</div>
<dl class="py method">
<dt id="kale.pipeline.video_transformer.VideoTransformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.video_transformer.VideoTransformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the cnntransformer independently on each frame of
each video and then concatenates the resulting sequence representations
of each video’s frames together into one long sequence per video
and applies a linformer-encoder to further contextualize each video
sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – a batch of videos with shape
(batch_size, fram_per_vid, channels, height, width)</p>
</dd>
</dl>
<dl class="simple">
<dt>Output Shape:</dt><dd><p>Returns output with shape (batch_size, final_seq_len, channels) where
the final sequence length will be out_height*out_width*fram_per_video
where out_height and out_width are the remaining spatial dimensions of the
CNNs output. Channels is the number of channels the CNN output has.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-kale.pipeline">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-kale.pipeline" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Haiping Lu, Shuo Zhou, Raivo Koot

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>