

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>kale.pipeline package &mdash; PyKale 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="kale.predict package" href="kale.predict.html" />
    <link rel="prev" title="kale.loaddata package" href="kale.loaddata.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> PyKale
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">kale</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="kale.html">kale package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="kale.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="kale.embed.html">kale.embed package</a></li>
<li class="toctree-l4"><a class="reference internal" href="kale.loaddata.html">kale.loaddata package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">kale.pipeline package</a></li>
<li class="toctree-l4"><a class="reference internal" href="kale.predict.html">kale.predict package</a></li>
<li class="toctree-l4"><a class="reference internal" href="kale.prepdata.html">kale.prepdata package</a></li>
<li class="toctree-l4"><a class="reference internal" href="kale.utils.html">kale.utils package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="kale.html#module-kale">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyKale</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="modules.html">kale</a> &raquo;</li>
        
          <li><a href="kale.html">kale package</a> &raquo;</li>
        
      <li>kale.pipeline package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/kale.pipeline.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="kale-pipeline-package">
<h1>kale.pipeline package<a class="headerlink" href="#kale-pipeline-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-kale.pipeline.da_systems">
<span id="kale-pipeline-da-systems-module"></span><h2>kale.pipeline.da_systems module<a class="headerlink" href="#module-kale.pipeline.da_systems" title="Permalink to this headline">¶</a></h2>
<p>Domain adaptation architectures should be indifferent to the task at hand: digits, toy datasets, recsys etc.
Take modules as input and organise them into an architecture.</p>
<p>From <a class="reference external" href="https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/models/architectures.py">https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/models/architectures.py</a></p>
<dl class="py class">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">BaseAdaptTrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">method</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">lambda_init</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">adapt_lambda</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">adapt_lr</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">nb_init_epochs</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">nb_adapt_epochs</span><span class="o">=</span><span class="default_value">50</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">init_lr</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">split_name</span><span class="o">=</span><span class="default_value">'V'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the loss of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – batches returned by the MultiDomainLoader.</p></li>
<li><p><strong>split_name</strong> (<em>str</em><em>, </em><em>optional</em>) – learning stage (one of [“T”, “V”, “Te”]).
Defaults to “V” for validation. “T” is for training and “Te” for test.
This is currently used only for naming the metrics used for logging.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a 3-element tuple with task_loss, adv_loss, log_metrics.
log_metrics should be a dictionary.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – children of this classes should implement this method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.configure_optimizers">
<code class="sig-name descname">configure_optimizers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’ key which value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains scheduler and its associated configuration.
It has five keys. The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR schduler</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="c1"># The unit of the scheduler&#39;s step size</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">&#39;reduce_on_plateau&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_loss&#39;</span> <span class="c1"># Metric to monitor</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If user only provides LR schedulers, then their configuration will set to default as shown above.</p>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.training_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">,</span>  <span class="c1"># or &#39;epoch&#39;</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_f1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.training_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.get_parameters_watch_list">
<code class="sig-name descname">get_parameters_watch_list</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.get_parameters_watch_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Update this list for parameters to watch while training (ie log with MLFlow)</p>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.method">
<em class="property">property </em><code class="sig-name descname">method</code><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.test_dataloader">
<code class="sig-name descname">test_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id1"><span class="problematic" id="id2">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
<li><p><a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.train_dataloader" title="kale.pipeline.da_systems.BaseAdaptTrainer.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.val_dataloader" title="kale.pipeline.da_systems.BaseAdaptTrainer.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_dataloader" title="kale.pipeline.da_systems.BaseAdaptTrainer.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Single or multiple PyTorch DataLoaders.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you don’t need to implement
this method.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.test_epoch_end">
<code class="sig-name descname">test_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">outputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> – List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Dict has the following optional keys:</p>
<ul class="simple">
<li><p>progress_bar -&gt; Dict for progress bar display. Must have only tensors.</p></li>
<li><p>log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc).</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Dict or OrderedDict</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, this won’t be called.</p>
</div>
<ul class="simple">
<li><p>The outputs here are strictly for logging or progress bar.</p></li>
<li><p>If you don’t need to display anything, don’t return anything.</p></li>
<li><p>If you want to manually set current step, specify it with the ‘step’ key in the ‘log’ Dict</p></li>
</ul>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">test_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">test_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span>

    <span class="n">test_acc_mean</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show test_loss and test_acc in progress bar but only log test_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">test_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="n">test_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">test_acc_mean</span> <span class="o">/=</span> <span class="n">i</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show test_loss and test_acc in progress bar but only log test_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.test_step">
<code class="sig-name descname">test_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">batch_nb</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test datasets used).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict or OrderedDict - passed to the <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_epoch_end" title="kale.pipeline.da_systems.BaseAdaptTrainer.test_epoch_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_epoch_end()</span></code></a> method.
If you defined <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code> it will go to that first.</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># all optional...</span>
    <span class="c1"># return whatever you need for the collation function test_epoch_end</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">({</span>
        <span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss_val</span><span class="p">,</span>
        <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_acc</span><span class="p">),</span> <span class="c1"># everything must be a tensor</span>
    <span class="p">})</span>

    <span class="c1"># return an optional dict</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>If you pass in multiple validation datasets, <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional
argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test datasets</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataset_idx</span><span class="p">):</span>
    <span class="c1"># dataset_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.train_dataloader">
<code class="sig-name descname">train_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement a PyTorch DataLoader for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Single PyTorch <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p>
</dd>
</dl>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id3"><span class="problematic" id="id4">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
<li><p><a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.train_dataloader" title="kale.pipeline.da_systems.BaseAdaptTrainer.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.training_step">
<code class="sig-name descname">training_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">batch_nb</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The most generic of training steps</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – the batch as returned by the MultiDomainLoader dataloader iterator:
2 tuples: (x_source, y_source), (x_target, y_target) in the unsupervised setting
3 tuples: (x_source, y_source), (x_target_labeled, y_target_labeled), (x_target_unlabeled, y_target_unlabeled) in the semi-supervised setting</p></li>
<li><p><strong>batch_nb</strong> (<em>int</em>) – id of the current batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>must contain a “loss” key with the loss to be used for back-propagation.</dt><dd><p>see pytorch-lightning for more details.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.val_dataloader">
<code class="sig-name descname">val_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id5"><span class="problematic" id="id6">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>It’s recommended that all data downloads and preparation happen in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.train_dataloader" title="kale.pipeline.da_systems.BaseAdaptTrainer.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.val_dataloader" title="kale.pipeline.da_systems.BaseAdaptTrainer.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.test_dataloader" title="kale.pipeline.da_systems.BaseAdaptTrainer.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Single or multiple PyTorch DataLoaders.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.validation_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.validation_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataset_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.validation_epoch_end">
<code class="sig-name descname">validation_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">outputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> – List of outputs you defined in <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.validation_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Dict or OrderedDict.
May have the following optional keys:</p>
<ul class="simple">
<li><p>progress_bar (dict for progress bar display; only tensors)</p></li>
<li><p>log (dict of metrics to add to logger; only tensors).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.validation_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, this won’t be called.</p>
</div>
<ul class="simple">
<li><p>The outputs here are strictly for logging or progress bar.</p></li>
<li><p>If you don’t need to display anything, don’t return anything.</p></li>
<li><p>If you want to manually set current step, you can specify the ‘step’ key in the ‘log’ dict.</p></li>
</ul>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">val_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">val_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>

    <span class="n">val_acc_mean</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show val_acc in progress bar but only log val_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">val_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="n">val_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">val_acc_mean</span> <span class="o">/=</span> <span class="n">i</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show val_loss and val_acc in progress bar but only log val_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseAdaptTrainer.validation_step">
<code class="sig-name descname">validation_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">batch_nb</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseAdaptTrainer.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple val datasets used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict or OrderedDict - passed to <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.validation_epoch_end" title="kale.pipeline.da_systems.BaseAdaptTrainer.validation_epoch_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_epoch_end()</span></code></a>.
If you defined <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step_end()</span></code> it will go to that first.</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">()</span>
<span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s1">&#39;validation_step_end&#39;</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># all optional...</span>
    <span class="c1"># return whatever you need for the collation function validation_epoch_end</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">({</span>
        <span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss_val</span><span class="p">,</span>
        <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_acc</span><span class="p">),</span> <span class="c1"># everything must be a tensor</span>
    <span class="p">})</span>

    <span class="c1"># return an optional dict</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>If you pass in multiple val datasets, validation_step will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation datasets</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataset_idx</span><span class="p">):</span>
    <span class="c1"># dataset_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer.validation_step" title="kale.pipeline.da_systems.BaseAdaptTrainer.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.BaseDANNLike">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">BaseDANNLike</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">critic</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">entropy_reg</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">adapt_reg</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">batch_reweighting</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseDANNLike" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer" title="kale.pipeline.da_systems.BaseAdaptTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.BaseAdaptTrainer</span></code></a></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseDANNLike.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">split_name</span><span class="o">=</span><span class="default_value">'V'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseDANNLike.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the loss of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – batches returned by the MultiDomainLoader.</p></li>
<li><p><strong>split_name</strong> (<em>str</em><em>, </em><em>optional</em>) – learning stage (one of [“T”, “V”, “Te”]).
Defaults to “V” for validation. “T” is for training and “Te” for test.
This is currently used only for naming the metrics used for logging.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a 3-element tuple with task_loss, adv_loss, log_metrics.
log_metrics should be a dictionary.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – children of this classes should implement this method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseDANNLike.get_parameters_watch_list">
<code class="sig-name descname">get_parameters_watch_list</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseDANNLike.get_parameters_watch_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Update this list for parameters to watch while training (ie log with MLFlow)</p>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseDANNLike.test_epoch_end">
<code class="sig-name descname">test_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">outputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseDANNLike.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> – List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Dict has the following optional keys:</p>
<ul class="simple">
<li><p>progress_bar -&gt; Dict for progress bar display. Must have only tensors.</p></li>
<li><p>log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc).</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Dict or OrderedDict</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, this won’t be called.</p>
</div>
<ul class="simple">
<li><p>The outputs here are strictly for logging or progress bar.</p></li>
<li><p>If you don’t need to display anything, don’t return anything.</p></li>
<li><p>If you want to manually set current step, specify it with the ‘step’ key in the ‘log’ Dict</p></li>
</ul>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">test_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">test_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span>

    <span class="n">test_acc_mean</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show test_loss and test_acc in progress bar but only log test_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">test_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="n">test_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">test_acc_mean</span> <span class="o">/=</span> <span class="n">i</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show test_loss and test_acc in progress bar but only log test_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseDANNLike.validation_epoch_end">
<code class="sig-name descname">validation_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">outputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseDANNLike.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> – List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Dict or OrderedDict.
May have the following optional keys:</p>
<ul class="simple">
<li><p>progress_bar (dict for progress bar display; only tensors)</p></li>
<li><p>log (dict of metrics to add to logger; only tensors).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, this won’t be called.</p>
</div>
<ul class="simple">
<li><p>The outputs here are strictly for logging or progress bar.</p></li>
<li><p>If you don’t need to display anything, don’t return anything.</p></li>
<li><p>If you want to manually set current step, you can specify the ‘step’ key in the ‘log’ dict.</p></li>
</ul>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">val_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">val_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>

    <span class="n">val_acc_mean</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show val_acc in progress bar but only log val_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">val_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="n">val_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">val_acc_mean</span> <span class="o">/=</span> <span class="n">i</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show val_loss and val_acc in progress bar but only log val_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.BaseMMDLike">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">BaseMMDLike</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">kernel_mul</span><span class="o">=</span><span class="default_value">2.0</span></em>, <em class="sig-param"><span class="n">kernel_num</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseMMDLike" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.BaseAdaptTrainer" title="kale.pipeline.da_systems.BaseAdaptTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.BaseAdaptTrainer</span></code></a></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseMMDLike.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">split_name</span><span class="o">=</span><span class="default_value">'V'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseMMDLike.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the loss of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – batches returned by the MultiDomainLoader.</p></li>
<li><p><strong>split_name</strong> (<em>str</em><em>, </em><em>optional</em>) – learning stage (one of [“T”, “V”, “Te”]).
Defaults to “V” for validation. “T” is for training and “Te” for test.
This is currently used only for naming the metrics used for logging.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a 3-element tuple with task_loss, adv_loss, log_metrics.
log_metrics should be a dictionary.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – children of this classes should implement this method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseMMDLike.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseMMDLike.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseMMDLike.test_epoch_end">
<code class="sig-name descname">test_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">outputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseMMDLike.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> – List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Dict has the following optional keys:</p>
<ul class="simple">
<li><p>progress_bar -&gt; Dict for progress bar display. Must have only tensors.</p></li>
<li><p>log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc).</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Dict or OrderedDict</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, this won’t be called.</p>
</div>
<ul class="simple">
<li><p>The outputs here are strictly for logging or progress bar.</p></li>
<li><p>If you don’t need to display anything, don’t return anything.</p></li>
<li><p>If you want to manually set current step, specify it with the ‘step’ key in the ‘log’ Dict</p></li>
</ul>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">test_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">test_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span>

    <span class="n">test_acc_mean</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show test_loss and test_acc in progress bar but only log test_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">test_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="n">test_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">test_acc_mean</span> <span class="o">/=</span> <span class="n">i</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show test_loss and test_acc in progress bar but only log test_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.BaseMMDLike.validation_epoch_end">
<code class="sig-name descname">validation_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">outputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.BaseMMDLike.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> – List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Dict or OrderedDict.
May have the following optional keys:</p>
<ul class="simple">
<li><p>progress_bar (dict for progress bar display; only tensors)</p></li>
<li><p>log (dict of metrics to add to logger; only tensors).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, this won’t be called.</p>
</div>
<ul class="simple">
<li><p>The outputs here are strictly for logging or progress bar.</p></li>
<li><p>If you don’t need to display anything, don’t return anything.</p></li>
<li><p>If you want to manually set current step, you can specify the ‘step’ key in the ‘log’ dict.</p></li>
</ul>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">val_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">val_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>

    <span class="n">val_acc_mean</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show val_acc in progress bar but only log val_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">val_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="n">val_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">val_acc_mean</span> <span class="o">/=</span> <span class="n">i</span>
    <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

    <span class="c1"># show val_loss and val_acc in progress bar but only log val_loss</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;progress_bar&#39;</span><span class="p">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc_mean</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.CDANtrainer">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">CDANtrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">critic</span></em>, <em class="sig-param"><span class="n">use_entropy</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">use_random</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_dim</span><span class="o">=</span><span class="default_value">1024</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.CDANtrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.BaseDANNLike" title="kale.pipeline.da_systems.BaseDANNLike"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.BaseDANNLike</span></code></a></p>
<p>Implements CDAN: Long, Mingsheng, et al. “Conditional adversarial domain adaptation.”
Advances in Neural Information Processing Systems. 2018.
<a class="reference external" href="https://papers.nips.cc/paper/7436-conditional-adversarial-domain-adaptation.pdf">https://papers.nips.cc/paper/7436-conditional-adversarial-domain-adaptation.pdf</a></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.CDANtrainer.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">split_name</span><span class="o">=</span><span class="default_value">'V'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.CDANtrainer.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the loss of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – batches returned by the MultiDomainLoader.</p></li>
<li><p><strong>split_name</strong> (<em>str</em><em>, </em><em>optional</em>) – learning stage (one of [“T”, “V”, “Te”]).
Defaults to “V” for validation. “T” is for training and “Te” for test.
This is currently used only for naming the metrics used for logging.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a 3-element tuple with task_loss, adv_loss, log_metrics.
log_metrics should be a dictionary.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – children of this classes should implement this method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.CDANtrainer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.CDANtrainer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.DANNtrainer">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">DANNtrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">critic</span></em>, <em class="sig-param"><span class="n">method</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.DANNtrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.BaseDANNLike" title="kale.pipeline.da_systems.BaseDANNLike"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.BaseDANNLike</span></code></a></p>
<p>This class implements the DANN architecture from
Ganin, Yaroslav, et al.
“Domain-adversarial training of neural networks.”
The Journal of Machine Learning Research (2016)
<a class="reference external" href="https://arxiv.org/abs/1505.07818">https://arxiv.org/abs/1505.07818</a></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.DANNtrainer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.DANNtrainer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.DANtrainer">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">DANtrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.DANtrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.BaseMMDLike" title="kale.pipeline.da_systems.BaseMMDLike"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.BaseMMDLike</span></code></a></p>
<p>This is an implementation of DAN
Long, Mingsheng, et al.
“Learning Transferable Features with Deep Adaptation Networks.”
International Conference on Machine Learning. 2015.
<a class="reference external" href="http://proceedings.mlr.press/v37/long15.pdf">http://proceedings.mlr.press/v37/long15.pdf</a>
code based on <a class="reference external" href="https://github.com/thuml/Xlearn">https://github.com/thuml/Xlearn</a>.</p>
</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.FewShotDANNtrainer">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">FewShotDANNtrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">critic</span></em>, <em class="sig-param"><span class="n">method</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.FewShotDANNtrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.BaseDANNLike" title="kale.pipeline.da_systems.BaseDANNLike"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.BaseDANNLike</span></code></a></p>
<p>Implements adaptations of DANN to the semi-supervised setting</p>
<p>naive: task classifier is trained on labeled target data, in addition to source
data.
MME: immplements Saito, Kuniaki, et al.
“Semi-supervised domain adaptation via minimax entropy.”
Proceedings of the IEEE International Conference on Computer Vision. 2019
<a class="reference external" href="https://arxiv.org/pdf/1904.06487.pdf">https://arxiv.org/pdf/1904.06487.pdf</a></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.FewShotDANNtrainer.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">split_name</span><span class="o">=</span><span class="default_value">'V'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.FewShotDANNtrainer.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the loss of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – batches returned by the MultiDomainLoader.</p></li>
<li><p><strong>split_name</strong> (<em>str</em><em>, </em><em>optional</em>) – learning stage (one of [“T”, “V”, “Te”]).
Defaults to “V” for validation. “T” is for training and “Te” for test.
This is currently used only for naming the metrics used for logging.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a 3-element tuple with task_loss, adv_loss, log_metrics.
log_metrics should be a dictionary.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – children of this classes should implement this method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.FewShotDANNtrainer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.FewShotDANNtrainer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.JANtrainer">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">JANtrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">kernel_mul</span><span class="o">=</span><span class="default_value">2.0, 2.0</span></em>, <em class="sig-param"><span class="n">kernel_num</span><span class="o">=</span><span class="default_value">5, 1</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.JANtrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.BaseMMDLike" title="kale.pipeline.da_systems.BaseMMDLike"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.BaseMMDLike</span></code></a></p>
<p>This is an implementation of JAN
Long, Mingsheng, et al.
“Deep transfer learning with joint adaptation networks.”
International Conference on Machine Learning, 2017.
<a class="reference external" href="https://arxiv.org/pdf/1605.06636.pdf">https://arxiv.org/pdf/1605.06636.pdf</a>
code based on <a class="reference external" href="https://github.com/thuml/Xlearn">https://github.com/thuml/Xlearn</a>.</p>
</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.Method">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">Method</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.Method" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>Lists the available methods.
Provides a few methods that group the methods by type.</p>
<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.CDAN">
<code class="sig-name descname">CDAN</code><em class="property"> = 'CDAN'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.CDAN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.CDAN_E">
<code class="sig-name descname">CDAN_E</code><em class="property"> = 'CDAN-E'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.CDAN_E" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.DAN">
<code class="sig-name descname">DAN</code><em class="property"> = 'DAN'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.DAN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.DANN">
<code class="sig-name descname">DANN</code><em class="property"> = 'DANN'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.DANN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.FSDANN">
<code class="sig-name descname">FSDANN</code><em class="property"> = 'FSDANN'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.FSDANN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.JAN">
<code class="sig-name descname">JAN</code><em class="property"> = 'JAN'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.JAN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.MME">
<code class="sig-name descname">MME</code><em class="property"> = 'MME'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.MME" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.Source">
<code class="sig-name descname">Source</code><em class="property"> = 'Source'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.Source" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.WDGRL">
<code class="sig-name descname">WDGRL</code><em class="property"> = 'WDGRL'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.WDGRL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="kale.pipeline.da_systems.Method.WDGRLMod">
<code class="sig-name descname">WDGRLMod</code><em class="property"> = 'WDGRLMod'</em><a class="headerlink" href="#kale.pipeline.da_systems.Method.WDGRLMod" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.Method.allow_supervised">
<code class="sig-name descname">allow_supervised</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.Method.allow_supervised" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.Method.is_cdan_method">
<code class="sig-name descname">is_cdan_method</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.Method.is_cdan_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.Method.is_dann_method">
<code class="sig-name descname">is_dann_method</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.Method.is_dann_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.Method.is_fewshot_method">
<code class="sig-name descname">is_fewshot_method</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.Method.is_fewshot_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.Method.is_mmd_method">
<code class="sig-name descname">is_mmd_method</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.Method.is_mmd_method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.ReverseLayerF">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">ReverseLayerF</code><a class="headerlink" href="#kale.pipeline.da_systems.ReverseLayerF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.function.Function</span></code></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.ReverseLayerF.backward">
<em class="property">static </em><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.ReverseLayerF.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs did <a class="reference internal" href="#kale.pipeline.da_systems.ReverseLayerF.forward" title="kale.pipeline.da_systems.ReverseLayerF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> return, and it should return as many
tensors, as there were inputs to <a class="reference internal" href="#kale.pipeline.da_systems.ReverseLayerF.forward" title="kale.pipeline.da_systems.ReverseLayerF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the
gradient w.r.t the given output, and each returned value should be the
gradient w.r.t. the corresponding input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#kale.pipeline.da_systems.ReverseLayerF.backward" title="kale.pipeline.da_systems.ReverseLayerF.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#kale.pipeline.da_systems.ReverseLayerF.forward" title="kale.pipeline.da_systems.ReverseLayerF.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.ReverseLayerF.forward">
<em class="property">static </em><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">alpha</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.ReverseLayerF.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.WDGRLtrainer">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">WDGRLtrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">critic</span></em>, <em class="sig-param"><span class="n">k_critic</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">beta_ratio</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.BaseDANNLike" title="kale.pipeline.da_systems.BaseDANNLike"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.BaseDANNLike</span></code></a></p>
<p>Implements WDGRL as described in
Shen, Jian, et al.
“Wasserstein distance guided representation learning for domain adaptation.”
Thirty-Second AAAI Conference on Artificial Intelligence. 2018.
<a class="reference external" href="https://arxiv.org/pdf/1707.01217.pdf">https://arxiv.org/pdf/1707.01217.pdf</a></p>
<p>This class also implements the asymmetric ($eta$) variant described in:
Wu, Yifan, et al.
“Domain adaptation with asymmetrically-relaxed distribution alignment.”
ICML (2019)
<a class="reference external" href="https://arxiv.org/pdf/1903.01689.pdf">https://arxiv.org/pdf/1903.01689.pdf</a></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainer.compute_loss">
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">split_name</span><span class="o">=</span><span class="default_value">'V'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainer.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Define the loss of the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – batches returned by the MultiDomainLoader.</p></li>
<li><p><strong>split_name</strong> (<em>str</em><em>, </em><em>optional</em>) – learning stage (one of [“T”, “V”, “Te”]).
Defaults to “V” for validation. “T” is for training and “Te” for test.
This is currently used only for naming the metrics used for logging.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a 3-element tuple with task_loss, adv_loss, log_metrics.
log_metrics should be a dictionary.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – children of this classes should implement this method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainer.configure_optimizers">
<code class="sig-name descname">configure_optimizers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainer.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’ key which value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains scheduler and its associated configuration.
It has five keys. The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR schduler</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="c1"># The unit of the scheduler&#39;s step size</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">&#39;reduce_on_plateau&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_loss&#39;</span> <span class="c1"># Metric to monitor</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If user only provides LR schedulers, then their configuration will set to default as shown above.</p>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#kale.pipeline.da_systems.WDGRLtrainer.training_step" title="kale.pipeline.da_systems.WDGRLtrainer.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">,</span>  <span class="c1"># or &#39;epoch&#39;</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_f1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainer.critic_update_steps">
<code class="sig-name descname">critic_update_steps</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainer.critic_update_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#kale.pipeline.da_systems.WDGRLtrainer.training_step" title="kale.pipeline.da_systems.WDGRLtrainer.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainer.training_step">
<code class="sig-name descname">training_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">batch_id</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainer.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The most generic of training steps</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – the batch as returned by the MultiDomainLoader dataloader iterator:
2 tuples: (x_source, y_source), (x_target, y_target) in the unsupervised setting
3 tuples: (x_source, y_source), (x_target_labeled, y_target_labeled), (x_target_unlabeled, y_target_unlabeled) in the semi-supervised setting</p></li>
<li><p><strong>batch_nb</strong> (<em>int</em>) – id of the current batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>must contain a “loss” key with the loss to be used for back-propagation.</dt><dd><p>see pytorch-lightning for more details.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="kale.pipeline.da_systems.WDGRLtrainerMod">
<em class="property">class </em><code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">WDGRLtrainerMod</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">critic</span></em>, <em class="sig-param"><span class="n">k_critic</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">beta_ratio</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">base_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainerMod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#kale.pipeline.da_systems.WDGRLtrainer" title="kale.pipeline.da_systems.WDGRLtrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">kale.pipeline.da_systems.WDGRLtrainer</span></code></a></p>
<p>Implements a modified version WDGRL as described in
Shen, Jian, et al.
“Wasserstein distance guided representation learning for domain adaptation.”
Thirty-Second AAAI Conference on Artificial Intelligence. 2018.
<a class="reference external" href="https://arxiv.org/pdf/1707.01217.pdf">https://arxiv.org/pdf/1707.01217.pdf</a></p>
<p>This class also implements the asymmetric ($eta$) variant described in:
Wu, Yifan, et al.
“Domain adaptation with asymmetrically-relaxed distribution alignment.”
ICML (2019)
<a class="reference external" href="https://arxiv.org/pdf/1903.01689.pdf">https://arxiv.org/pdf/1903.01689.pdf</a></p>
<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainerMod.configure_optimizers">
<code class="sig-name descname">configure_optimizers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainerMod.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’ key which value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains scheduler and its associated configuration.
It has five keys. The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR schduler</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="c1"># The unit of the scheduler&#39;s step size</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">&#39;reduce_on_plateau&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_loss&#39;</span> <span class="c1"># Metric to monitor</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If user only provides LR schedulers, then their configuration will set to default as shown above.</p>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#kale.pipeline.da_systems.WDGRLtrainerMod.training_step" title="kale.pipeline.da_systems.WDGRLtrainerMod.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <a class="reference internal" href="#kale.pipeline.da_systems.WDGRLtrainerMod.optimizer_step" title="kale.pipeline.da_systems.WDGRLtrainerMod.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">,</span>  <span class="c1"># or &#39;epoch&#39;</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_f1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainerMod.critic_update_steps">
<code class="sig-name descname">critic_update_steps</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainerMod.critic_update_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainerMod.optimizer_step">
<code class="sig-name descname">optimizer_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">current_epoch</span></em>, <em class="sig-param"><span class="n">batch_nb</span></em>, <em class="sig-param"><span class="n">optimizer</span></em>, <em class="sig-param"><span class="n">optimizer_i</span></em>, <em class="sig-param"><span class="n">second_order_closure</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainerMod.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to adjust the default way the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.
By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example
once per optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – Current epoch</p></li>
<li><p><strong>batch_idx</strong> – Index of current batch</p></li>
<li><p><strong>optimizer</strong> – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> – If you used multiple optimizers this indexes into that list.</p></li>
<li><p><strong>second_order_closure</strong> – closure for second order methods</p></li>
<li><p><strong>on_tpu</strong> – true if TPU backward is required</p></li>
<li><p><strong>using_native_amp</strong> – True if using native amp</p></li>
<li><p><strong>using_lbfgs</strong> – True if the matching optimizer is lbfgs</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">second_order_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">second_order_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># update generator opt every 2 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># update discriminator opt every 4 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># ...</span>
    <span class="c1"># add as many optimizers as you want</span>
</pre></div>
</div>
<p>Here’s another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span>
                    <span class="n">optimizer_idx</span><span class="p">,</span> <span class="n">second_order_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># warm up lr</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_zero_grad()</span></code>
model hook don’t forget to add the call to it before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code> yourself.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="kale.pipeline.da_systems.WDGRLtrainerMod.training_step">
<code class="sig-name descname">training_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">batch_id</span></em>, <em class="sig-param"><span class="n">optimizer_idx</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.WDGRLtrainerMod.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The most generic of training steps</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>tuple</em>) – the batch as returned by the MultiDomainLoader dataloader iterator:
2 tuples: (x_source, y_source), (x_target, y_target) in the unsupervised setting
3 tuples: (x_source, y_source), (x_target_labeled, y_target_labeled), (x_target_unlabeled, y_target_unlabeled) in the semi-supervised setting</p></li>
<li><p><strong>batch_nb</strong> (<em>int</em>) – id of the current batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>must contain a “loss” key with the loss to be used for back-propagation.</dt><dd><p>see pytorch-lightning for more details.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="kale.pipeline.da_systems.create_dann_like">
<code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">create_dann_like</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">method</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#kale.pipeline.da_systems.Method" title="kale.pipeline.da_systems.Method">kale.pipeline.da_systems.Method</a></span></em>, <em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">critic</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">train_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.create_dann_like" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="kale.pipeline.da_systems.create_fewshot_trainer">
<code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">create_fewshot_trainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">method</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#kale.pipeline.da_systems.Method" title="kale.pipeline.da_systems.Method">kale.pipeline.da_systems.Method</a></span></em>, <em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="n">critic</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">train_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.create_fewshot_trainer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="kale.pipeline.da_systems.create_mmd_based">
<code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">create_mmd_based</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">method</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#kale.pipeline.da_systems.Method" title="kale.pipeline.da_systems.Method">kale.pipeline.da_systems.Method</a></span></em>, <em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">feature_extractor</span></em>, <em class="sig-param"><span class="n">task_classifier</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">train_params</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.create_mmd_based" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="kale.pipeline.da_systems.get_aggregated_metrics">
<code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">get_aggregated_metrics</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name_list</span></em>, <em class="sig-param"><span class="n">metric_outputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.get_aggregated_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="kale.pipeline.da_systems.get_aggregated_metrics_from_dict">
<code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">get_aggregated_metrics_from_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_metric_dict</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.get_aggregated_metrics_from_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="kale.pipeline.da_systems.get_metrics_from_parameter_dict">
<code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">get_metrics_from_parameter_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameter_dict</span></em>, <em class="sig-param"><span class="n">device</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.get_metrics_from_parameter_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="kale.pipeline.da_systems.set_requires_grad">
<code class="sig-prename descclassname">kale.pipeline.da_systems.</code><code class="sig-name descname">set_requires_grad</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">requires_grad</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#kale.pipeline.da_systems.set_requires_grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-kale.pipeline">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-kale.pipeline" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="kale.predict.html" class="btn btn-neutral float-right" title="kale.predict package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="kale.loaddata.html" class="btn btn-neutral float-left" title="kale.loaddata package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Haiping Lu, Shuo Zhou, Raivo Koot

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>